---
layout: post
title: Druid..!!
---

# Druid Overview
## ㅁ Druid's main features
 - Columnar storage format
 - Scalable distributed system
 - Massively parallel processing
 - Realtime or batch ingestion
 - Self-healing, self-balancing, easy to operate
 - Cloud-native, fault-tolerant architecture that won't lose data
 - Indexes for quick filtering
 - Approximate algorithms
 - Automatic summarization at ingest time
 



<br/>
## ㅁ When should I use Druid?
 - Insert rates are very high, but updates are less common. <br/>
 ( 사실 Insert 만 있어야함. update 는 안됌. 모든 Data 는 immutable 한 상태라 보는 것이 맞음 )
 - Most of your queries are aggregation and reporting queries ("group by" queries). You may also have searching and scanning queries. <br/>
 group by, filtering 을 구현하기에 좋다
 - You are targeting query latencies of 100ms to a few seconds. <br/>
( 100ms ~ 수초 이내에 query response 가 요구될 때 )
 - Your data has a time component <br/>
 ( 이것도 필수임 )
  - 

## ㅁ Architecture
### Overview
![_config.yml]({{ site.baseurl }}/images/druid/architecture_1.png) <br/>
### Storage
#### Segments
 - Default index 기준으로 저장함. ( 일단 timestamp 는 무조건.. ㅎㅎ )
 - Multi-value Columns 도 가능함 ( 여러개 들어가도 Bitmap indexing 이 가능하기 때문)
``` javascript
1: Dictionary that encodes column values
  {
    "Justin Bieber": 0,
    "Ke$ha":         1
  }

2: Column data
  [0,
   [0,1],  <--Row value of multi-value column can have array of values
   1,
   1]

3: Bitmaps - one for each unique value
  value="Justin Bieber": [1,1,0,0]
  value="Ke$ha":         [0,1,1,1]
                            ^
                            |
                            |
    Multi-value column has multiple non-zero entries
```
<br/>
##### Data Structures
![_config.yml]({{ site.baseurl }}/images/druid/data_structure.png) <br/>
 - 기본적으로 Timestamp, Dimensions, Metrics 가 있음
 - Dimensioons 컬럼에 의해 group-by, filter operation 이 일어남

<br/>
##### Segment Components
 - `version.bin` : 4bytes 로 버전을 나타냄
 - `meta.smoosh` : meta data ( 다른 smoosh 파일에 대한 )
 - `XXXXX.smoosh` : 데이터의 minimized 된 파일. 데이터의 각 열에 대한 개별 파일과 Segment 에 대한 extra metadata 가 있는 index.drd 파일이 있음

<br/>
##### Format of a column
```text
    1. A Jackson-serialized ColumnDescriptor
    2. The rest of the binary for the column
```

<br/>
##### Sharding
Segment 의 form 은 interval 별 block 이다. 이 block 은 `shardSpec`에 의존하고 druid의 queries 는 이 block 이 완성되어야만 완료된다. ( 즉, block 이 만들어 지기 전까지는 해당 interval 에 대해선 query 가 안날라 간다는 의미인듯 함 )
```text
sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_0

sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_1

sampleData_2011-01-01T02:00:00:00Z_2011-01-01T03:00:00:00Z_v1_2
```


### Nodes Types
#### Historical

<br/>
<br/>
<br/>
#### Broker

<br/>
<br/>
<br/>
#### Coordinator
 - HTTP endpoints supported
 - primarily responsible for `segment management and distribution`
 - 주기적으로 돌면서 적절한 action 을 취함.
 - broker / historical nodes 와 비슷하게 zookeeper 와 연결되어 현재 클러스터의 정보를 얻는다.
 - 또한 available segments 와 rules 정보를 위해 metastore 와도 connection을 맺고 있다. 
 - 역할 : Cleaning Up Segments, 

<br/>
<br/>
<br/>
#### Indexing Service
##### Overlord

<br/>
##### MiddleManager

<br/>
##### Peons

<br/>


### Dependencis
#### Deep Storage
 - Types : Local Mount, S3-compatible, HDFS, etc..
<br/>

#### Metadata Storage
 - Types : derby ( for not production), MySQL, PostgreSQL
 - Metadata Storage Tables
     + Segments Table
         * used column : 
         * payload column : 
         ```json 
            {
             "dataSource":"wikipedia",
             "interval":"2012-05-23T00:00:00.000Z/2012-05-24T00:00:00.000Z",
             "version":"2012-05-24T00:10:00.046Z",
             "loadSpec":{
                "type":"s3_zip",
                "bucket":"bucket_for_segment",
                "key":"path/to/segment/on/s3"
             },
             "dimensions":"comma-delimited-list-of-dimension-names",
             "metrics":"comma-delimited-list-of-metric-names",
             "shardSpec":{"type":"none"},
             "binaryVersion":9,
             "size":size_of_segment,
             "identifier":"wikipedia_2012-05-23T00:00:00.000Z_2012-05-24T00:00:00.000Z_2012-05-23T00:10:00.046Z"
            }
         ```
         * asdf
#### Zookeeper


## ㅁ Datasources and segments

## ㅁ Query processing

## ㅁ External Dependencies
####
